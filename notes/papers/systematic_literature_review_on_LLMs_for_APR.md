# **A Systematic Literature Review on Large Language Models for Automated Program Repair (12/05/2024)**

## Abstract
- First systematic literature review to summarize the applications of LLMs in APR between 2020 and 2024.
- Three types of utilization strategies for LLM deployment in APR.
- Critical aspects of integrating LLMs into APR research.

**Keywords:** Large Language Model, Automated Program Repair, LLM4APR

## BACKGROUND AND RELATED WORK

### 2.1 Automated Program Repair
- Repair Workflow:
    1. The **fault localization** phase identifies suspicious code elements that need to be fixed.
    2. The **patch generation** phase generates program variants.
    3. The patch validation phase utilizes available test suites as the oracle to identify correct patches .

- Repair Techniques:
    1. **Heuristic-based APR** utilizes genetic programming to explore the search space of the correct patch (GenProg).
    2. **Constraint-based APR** treats program repair as a constraint-solving task.
    3. **Pattern-based APR** utilizes pre-defined repair templates that are usually hand-crafted.
    4. **Learning-based APR** treats patch generation as a neural machine translation task.

### 2.2 Large Language Models
- LLMs refer to advanced AI models that undergo extensive pre-training on large amounts of text enhance their capabilities. They feature an enormous number of parameters, far exceeding the scale of traditional DL models, thus enabling them to assist in a wide range of tasks.
- In 1986, Recurrent Neural Networks (RNNs) were introduced. In 1997, RNNs evolved to Long ShortTerm Memory Networks (LSTMs) which then evolved to Transformers in 2017. LSTMs represent the foundation of LLMs.

## 5. RQ2: WHICH POPULAR LLMS HAVE BEEN APPLIED TO SUPPORT APR?

### 5.2 What approaches are employed to optimize LLMs for program repair?
- Αdaptation strategies in LLM-based APR research¨
    1. **Fine-tuning:** LLMs are further trained on a smaller, task-specific dataset. This allows LLMs to adjust their weights and biases through supervised learning.
    2. **Few-shot learning:** Τhe ability of LLMs to learn or adapt to new tasks with a very limited amount of data—often only a few examples. We use examples to help LLMs understand the targeted task and generate appropriate responses without any explicit retraining or fine-tuning.
    3. **Zero-shot Learning:** Zero-shot learning takes the concept of few-shot learning even further by requiring LLMs to perform program repair without any explicit examples.

## 6. RQ3: WHAT REPAIR SCENARIOS HAVE BEEN FACILITATED BY LLMS?
- **Semantic bugs:** Logical errors in a syntactically correct program; the code does not do what the programmer intends.
- **Security Vulnerabilities:** Flaw, bug, or weakness in the design, implementation, operation, or management of software. When exploited, vulnerabilities can lead to unauthorized access, manipulation, or disruption of the software’s intended functionality.
- **Static Warnings:** Automated alerts generated by static tools that examine code to identify potential errors, inefficiencies, or security vulnerabilities without executing the program.
- **Syntax Errors:** Parsing mistakes that occur when code does not follow the rules or grammar of the programming language, such as invalid statements and expressions.
- **Type Errors:** An operation or function is applied to an object of an inappropriate type.
- **Programming Problems:** Incorrect solutions to programming problems from competition platforms, such as LeetCode.
- **Performance Bugs:** Inefficient code snippets that do not interfere with functionality but lead to unnecessary time and resource consumption
- **Hardware Bugs:** Security-relevant bugs in hardware designs.
- **Smart Contracts:** Self-executing contracts on a blockchain network with the terms of the agreement directly written into code. I
- **Misc:** Crash Bugs, API Misuse, Web UI, Translation Bugs, Test Repair, Motion Planning Algorithm, Software Formal Proof, Github Issue, Code Review Refinement

# 7 RQ4: WHAT KEY FACTORS CONTRIBUTE TO THE INTEGRATION OF LLMS FOR APR?

## 7.2 What input forms are software bugs transformed into when utilizing LLMs?
1. **Raw Buf-fixing Input:** Translates a sentence from one source language (i.e., buggycode) to another target language (i.e., fixed code). 
2. **Prompt Input:** The prompt concatenates different input components with some prefixed prompt, thus effectively bridging the gap between pre-trained tasks and the APR downstream task.
3. **Mask Input:** Masks the buggy code and queries LLMs to fill the masks with the correct code tokens.
4. **Conversation-Style Representation:** Extends the prompt input with feedback-driven chats like humans.
5. **Structure-Aware Input:** Represents source code as syntactic structures, such as Abstract Syntax Trees (ASTs).

## 8 CHALLENGES & OPPORTUNITIES OF LLM-BASED APR
- APR as a Part of Fully Autonomous Programming
- More Attention about Repair Costs of LLMs
- Human Study with LLMs
- Exploring More and Rare Repair Scenarions
- Integration with Off-the-Shelf APR
- Data Leakage Issue