{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Master Class\n",
    "I will be using this notebook to take notes on the [LangChain Master Class For Beginners 2024](https://www.youtube.com/watch?v=yF9kGESAi3M) video. LangChain is a very good starting point for getting in touch with OpenAI's API and is significantly easier to use. This tutorial starts from basic features like interacting with an LLM or designing a real time conversation with it and progressively works its way towards more complex features, like chaining and ReAct agents. Let's start with the basics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Interacting with chat models\n",
    "Here, we learn the library's basics, like interacting with models like ChatGPT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat model basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Create the model\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# Invoke the model with a message\n",
    "result = model.invoke(\"What is 81 divided by 9?\")\n",
    "print(\"Full result:\")\n",
    "print(result)\n",
    "print(\"Content only:\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Create the model\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# SystemMessage: Message to manage AI behavior\n",
    "# HumanMessage: Message from a human to the AI model\n",
    "messages = [\n",
    "    SystemMessage(content=\"Solve the following math problems\"),\n",
    "    HumanMessage(content=\"What is 81 divided by 9?\"),\n",
    "]\n",
    "\n",
    "# Invoke the model with a message\n",
    "result = model.invoke(messages)\n",
    "print(f\"ChatGPT-4o-mini: {result.content}\")\n",
    "\n",
    "# # AIMessage: Response from an AI\n",
    "# messages = [\n",
    "#     SystemMessage(content=\"Solve the following math problems\"),\n",
    "#     HumanMessage(content=\"What is 81 divided by 9?\"),\n",
    "#     AIMessage(content=\"81 divided by 9 is 9.\"),\n",
    "#     HumanMessage(content=\"What is 10 times 5?\"),\n",
    "# ]\n",
    "# # Invoke the model with a message\n",
    "# result = model.invoke(messages)\n",
    "# print(f\"ChatGPT-4o-mini: {result.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real time conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Create the model\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "system_mesage = SystemMessage(content=\"You are a helpful AI assistant that answers questions in 2-3 sentences.\")\n",
    "chat_history.append(system_mesage)\n",
    "\n",
    "while True:\n",
    "    # Read user input\n",
    "    query = input(\"You: \")\n",
    "    if query.lower() == \"exit\":\n",
    "        break\n",
    "    chat_history.append(HumanMessage(content=query))\n",
    "\n",
    "    # Get AI response using history\n",
    "    result = model.invoke(chat_history)\n",
    "    response = result.content\n",
    "    chat_history.append(AIMessage(content=response))\n",
    "    print(f\"ChatGPT-4o-mini: {response}\")\n",
    "\n",
    "print(\"----- Message History -----\")\n",
    "for message in chat_history:\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prompt templates\n",
    "Now that we got the hang of interacting with a chat model, let's understand the basics of prompt templates and how to use them effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# # Template using a template string\n",
    "# template = \"Tell me a joke about {topic}.\"\n",
    "# prompt_template = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# print(\"-----Prompt from Template-----\")\n",
    "# prompt = prompt_template.invoke({\"topic\": \"cats\"})\n",
    "# print(prompt)\n",
    "\n",
    "\n",
    "\n",
    "# # Template using multiple placeholders\n",
    "# template = \"\"\"You are a helpful assistant.\n",
    "# Human: Tell me a {adjective} story about a {animal}.\n",
    "# Assistant:\"\"\"\n",
    "# prompt_template = ChatPromptTemplate.from_template(template)\n",
    "# prompt = prompt_template.invoke({\"adjective\": \"funny\", \"animal\": \"panda\"})\n",
    "# print(\"-----Prompt from Template-----\")\n",
    "# print(prompt)\n",
    "\n",
    "\n",
    "\n",
    "# System and human messages\n",
    "messages = [\n",
    "    (\"system\", \"You are a comedian who tells jokes about {topic}.\"),\n",
    "    (\"human\", \"Tell me {joke_count} jokes\"),\n",
    "]\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(messages)\n",
    "prompt = prompt_template.invoke({\"topic\": \"lawyers\", \"joke_count\": 3})\n",
    "print(\"-----Prompt from Template-----\")\n",
    "print(prompt)\n",
    "\n",
    "\n",
    "\n",
    "# #! THIS DOES NOT WORK!\n",
    "# messages = [\n",
    "#     (\"system\", \"You are a comedian who tells jokes about {topic}.\"),\n",
    "#     HumanMessage(content=\"Tell me {joke_count} jokes\"),\n",
    "# ]\n",
    "\n",
    "# prompt_template = ChatPromptTemplate.from_messages(messages)\n",
    "# prompt = prompt_template.invoke({\"topic\": \"lawyers\", \"joke_count\": 3})\n",
    "# print(\"-----Prompt from Template-----\")\n",
    "# print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt template with chat model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Create the model\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# Template using a template string\n",
    "print(\"-----Prompt from template-----\")\n",
    "template = \"Tell me a joke about {topic}.\"\n",
    "prompt_template = ChatPromptTemplate.from_template(template)\n",
    "prompt = prompt_template.invoke({\"topic\": \"cats\"})\n",
    "result = model.invoke(prompt)\n",
    "response = result.content\n",
    "print(f\"ChatGPT-4o-mini: {response}\")\n",
    "\n",
    "\n",
    "# Template using multiple placeholders\n",
    "print(\"-----Prompt from template with multiple placeholders-----\")\n",
    "template = \"\"\"You are a helpful assistant.\n",
    "Human: Tell me a {adjective} story about a {animal}.\n",
    "Assistant:\"\"\"\n",
    "prompt_template = ChatPromptTemplate.from_template(template)\n",
    "prompt = prompt_template.invoke({\"adjective\": \"funny\", \"animal\": \"panda\"})\n",
    "result = model.invoke(prompt)\n",
    "response = result.content\n",
    "print(f\"ChatGPT-4o-mini: {response}\")\n",
    "\n",
    "# Template with system and human messages\n",
    "print(\"-----Prompt from template with system and human messages-----\")\n",
    "messages = [\n",
    "    (\"system\", \"You are a comedian who tells jokes about {topic}.\"),\n",
    "    (\"human\", \"Tell me {joke_count} jokes\"),\n",
    "]\n",
    "prompt_template = ChatPromptTemplate.from_messages(messages)\n",
    "prompt = prompt_template.invoke({\"topic\": \"lawyers\", \"joke_count\": 3})\n",
    "result = model.invoke(prompt)\n",
    "response = result.content\n",
    "print(f\"ChatGPT-4o-mini: {response}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Chains\n",
    "Now that we are familiar with chat models and prompts, it is time to build chains to automate tasks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Create the model\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# Define prompt templates\n",
    "messages = [\n",
    "    (\"system\", \"You are a comedian who tells jokes about {topic}.\"),\n",
    "    (\"human\", \"Tell me {joke_count} jokes\"),\n",
    "]\n",
    "prompt_template = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "# Create a chain and make output readable\n",
    "chain = prompt_template | model | StrOutputParser()\n",
    "\n",
    "# Run the chain\n",
    "result = chain.invoke({\"topic\": \"dogs\", \"joke_count\": 3})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chain parallels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableParallel, RunnableLambda\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Create the model\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# Define prompt templates\n",
    "messages = [\n",
    "    (\"system\", \"You are an expert product reviewer.\"),\n",
    "    (\"human\", \"List me the main features of the product {product_name}\"),\n",
    "]\n",
    "prompt_template = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "def analyze_pros(features):\n",
    "    messages = [\n",
    "        (\"system\", \"You are an expert product reviewer.\"),\n",
    "        (\"human\", \"Given these features: {features}, list the pros of these features.\"),\n",
    "    ]\n",
    "    pros_template = ChatPromptTemplate.from_messages(messages)\n",
    "    return pros_template.format_prompt(features=features)\n",
    "\n",
    "# Pros branch\n",
    "pros_branch_chain = (\n",
    "    RunnableLambda(lambda x: analyze_pros(x)) | model | StrOutputParser()\n",
    ")\n",
    "\n",
    "def analyze_cons(features):\n",
    "    messages = [\n",
    "        (\"system\", \"You are an expert product reviewer.\"),\n",
    "        (\"human\", \"Given these features: {features}, list the cons of these features.\"),\n",
    "    ]\n",
    "    cons_template = ChatPromptTemplate.from_messages(messages)\n",
    "    return cons_template.format_prompt(features=features)\n",
    "\n",
    "# Cons branch\n",
    "cons_branch_chain = (\n",
    "    RunnableLambda(lambda x: analyze_cons(x)) | model | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Combine pros and cons into a final review\n",
    "def combine_pros_cons(pros, cons):\n",
    "    return f\"Pros:\\n{pros}\\n\\nCons:\\n{cons}\"\n",
    "\n",
    "# Create a chain and make output readable\n",
    "chain = (\n",
    "    prompt_template\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    "    | RunnableParallel(branches={\"pros\": pros_branch_chain, \"cons\": cons_branch_chain})\n",
    "    | RunnableLambda(lambda x: combine_pros_cons(x[\"branches\"][\"pros\"], x[\"branches\"][\"cons\"]))\n",
    ")\n",
    "\n",
    "# Run the chain\n",
    "result = chain.invoke({\"product_name\": \"The Latest iPhone\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableBranch\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Create the model\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# Define prompt templates\n",
    "positive_messages = [\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"human\", \"Generate a thank you note for this positive feedback: {feedback}\"),\n",
    "]\n",
    "positive_feedback_template = ChatPromptTemplate.from_messages(positive_messages)\n",
    "\n",
    "negative_messages = [\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"human\", \"Generate a response addressing this negative feedback: {feedback}\"),\n",
    "]\n",
    "negative_feedback_template = ChatPromptTemplate.from_messages(negative_messages)\n",
    "\n",
    "neutral_messages = [\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"human\", \"Generate a request for more details for this neutral feedback: {feedback}\"),\n",
    "]\n",
    "neutral_feedback_template = ChatPromptTemplate.from_messages(neutral_messages)\n",
    "\n",
    "escalate_messages = [\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"human\", \"Generate a message to escalate this feedback to a human agent: {feedback}\"),\n",
    "]\n",
    "escalate_feedback_template = ChatPromptTemplate.from_messages(escalate_messages)\n",
    "\n",
    "# Define a feedback classification template\n",
    "classification_messages = [\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"human\", \"Classify the sentiment of this feedback as positive, negative, neutral, or escalate: {feedback}\"),\n",
    "]\n",
    "classification_template = ChatPromptTemplate.from_messages(classification_messages)\n",
    "\n",
    "# Define the runnable branches\n",
    "branches = RunnableBranch(\n",
    "    (lambda x: \"positive\" in x, positive_feedback_template | model | StrOutputParser()),\n",
    "    (lambda x: \"negative\" in x, negative_feedback_template | model | StrOutputParser()),\n",
    "    (lambda x: \"neutral\" in x, neutral_feedback_template | model | StrOutputParser()),\n",
    "    escalate_feedback_template | model | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Create a chain which branches according to the LLM's classification\n",
    "chain = classification_template | model | StrOutputParser() | branches\n",
    "\n",
    "# Run the chain with an example review\n",
    "# Good review - \"The product is excellent. I really enjoyed using it and found it very helpful.\"\n",
    "# Bad review - \"The product is terrible. It broke after just one use and the quality is very poor.\"\n",
    "# Neutral review - \"The product is okay. It works as expected but nothing exceptional.\"\n",
    "# Default - \"I'm not sure about the product yet. Can you tell me more about its features and benefits?\"\n",
    "print (\"-----Good review-----\")\n",
    "review = \"The product is excellent. I really enjoyed using it and found it very helpful.\"\n",
    "result = chain.invoke({\"feedback\": review})\n",
    "print(result)\n",
    "\n",
    "print (\"-----Bad review-----\")\n",
    "review = \"The product is terrible. It broke after just one use and the quality is very poor.\"\n",
    "result = chain.invoke({\"feedback\": review})\n",
    "print(result)\n",
    "\n",
    "print (\"-----Neutral review-----\")\n",
    "review = \"The product is okay. It works as expected but nothing exceptional.\"\n",
    "result = chain.invoke({\"feedback\": review})\n",
    "print(result)\n",
    "\n",
    "print (\"-----Default-----\")\n",
    "review = \"I'm not sure about the product yet. Can you tell me more about its features and benefits?\"\n",
    "result = chain.invoke({\"feedback\": review})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Agents & Tools\n",
    "Finally, let's learn about agents, how they work, and how to build custom tools to enhance their capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import (\n",
    "    AgentExecutor,\n",
    "    create_react_agent,\n",
    ")\n",
    "from langchain_core.tools import Tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# This function will be used as a tool that returns the current time\n",
    "def get_current_time(*args, **kwargs):\n",
    "    \"\"\"Returns the current military time in HH:MM:SS format.\"\"\"\n",
    "    from datetime import datetime\n",
    "    return datetime.now().strftime(\"%H:%M:%S\")\n",
    "\n",
    "# List of tools available to the agent\n",
    "tools = []\n",
    "current_time_tool = Tool(\n",
    "    name=\"Time\",\n",
    "    func=get_current_time,\n",
    "    description=\"Useful for when you need to know the current time\"\n",
    ")\n",
    "tools.append(current_time_tool)\n",
    "\n",
    "# Pull the prompt template from the hub. This prompt follows ReAct = Reason and Action\n",
    "# https://smith.langchain.com/hub/hwchase17/react\n",
    "prompt = hub.pull(\"hwchase17/react\")\n",
    "\n",
    "# Create the model\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# Create a ReAct agent\n",
    "agent = create_react_agent(llm=model, tools=tools, prompt=prompt, stop_sequence=True)\n",
    "\n",
    "# Create the agent executor\n",
    "agent_executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "# Run the agent\n",
    "response = agent_executor.invoke({\"input\" : \"What time is it?\"})\n",
    "print(f\"ChatGPT-4o-mini: {response['output']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReAct agent chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_structured_chat_agent\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_core.tools import Tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "# Define Tools\n",
    "def get_current_time(*args, **kwargs):\n",
    "    \"\"\"Returns the current time in H:MM AM/PM format.\"\"\"\n",
    "    import datetime\n",
    "\n",
    "    now = datetime.datetime.now()\n",
    "    return now.strftime(\"%I:%M %p\")\n",
    "\n",
    "\n",
    "def search_wikipedia(query):\n",
    "    \"\"\"Searches Wikipedia and returns the summary of the first result.\"\"\"\n",
    "    from wikipedia import summary\n",
    "\n",
    "    try:\n",
    "        # Limit to two sentences for brevity\n",
    "        return summary(query, sentences=2)\n",
    "    except:\n",
    "        return \"I couldn't find any information on that.\"\n",
    "\n",
    "\n",
    "# Define the tools that the agent can use\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Time\",\n",
    "        func=get_current_time,\n",
    "        description=\"Useful for when you need to know the current time.\",\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Wikipedia\",\n",
    "        func=search_wikipedia,\n",
    "        description=\"Useful for when you need to know information about a topic.\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Load the correct JSON Chat Prompt from the hub\n",
    "prompt = hub.pull(\"hwchase17/structured-chat-agent\")\n",
    "\n",
    "# Initialize a ChatOpenAI model\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "# Create a structured Chat Agent with Conversation Buffer Memory\n",
    "# ConversationBufferMemory stores the conversation history, allowing the agent to maintain context across interactions\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# create_structured_chat_agent initializes a chat agent designed to interact using a structured prompt and tools\n",
    "# It combines the language model (llm), tools, and prompt to create an interactive agent\n",
    "agent = create_structured_chat_agent(llm=llm, tools=tools, prompt=prompt)\n",
    "\n",
    "# AgentExecutor is responsible for managing the interaction between the user input, the agent, and the tools\n",
    "# It also handles memory to ensure context is maintained throughout the conversation\n",
    "agent_executor = AgentExecutor.from_agent_and_tools(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True,\n",
    "    memory=memory,  # Use the conversation memory to maintain context\n",
    "    handle_parsing_errors=True,  # Handle any parsing errors gracefully\n",
    ")\n",
    "\n",
    "# Initial system message to set the context for the chat\n",
    "# SystemMessage is used to define a message from the system to the agent, setting initial instructions or context\n",
    "initial_message = \"You are an AI assistant that can provide helpful answers using available tools.\\nIf you are unable to answer, you can use the following tools: Time and Wikipedia.\"\n",
    "memory.chat_memory.add_message(SystemMessage(content=initial_message))\n",
    "\n",
    "# Chat Loop to interact with the user\n",
    "while True:\n",
    "    user_input = input(\"User: \")\n",
    "    if user_input.lower() == \"exit\":\n",
    "        break\n",
    "\n",
    "    # Add the user's message to the conversation memory\n",
    "    memory.chat_memory.add_message(HumanMessage(content=user_input))\n",
    "\n",
    "    # Invoke the agent with the user input and the current chat history\n",
    "    response = agent_executor.invoke({\"input\": user_input})\n",
    "    print(\"Bot:\", response[\"output\"])\n",
    "\n",
    "    # Add the agent's response to the conversation memory\n",
    "    memory.chat_memory.add_message(AIMessage(content=response[\"output\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
